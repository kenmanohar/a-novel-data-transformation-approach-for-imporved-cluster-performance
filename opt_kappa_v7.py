# -*- coding: utf-8 -*-
"""opt_kappa v7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18yCyN9aEL-_obvKP7ixrT7ESIvNKLzK9

Version 7

Advanced way to find "robust averages" for numerical features
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Set pandas options to display values without scientific notation
pd.set_option('display.float_format', '{:.4f}'.format)

# # Auto dataset
# # All columns = ['mpg', 'cylinders', 'acc', 'year']

# data = 'https://raw.githubusercontent.com/kenmanohar/stat6005/main/auto.csv'
# df = pd.read_csv(data)

# # Drop rows with null values
# df.dropna(inplace=True)
# df.reset_index(drop=True, inplace=True)

# # # Choose attributes
# #subset_columns = ['cylinders', 'acc', 'year', 'mpg']
# subset_columns = ['year', 'mpg']

# df = df[subset_columns].copy()

# # Set label column
# label = 'mpg'

# # Extract attributes
# attributes = df.columns.tolist()
# attributes.remove(label)

# # Insurance dataset
# # All columns = ['index', 'PatientID', 'age', 'gender', 'bmi', 'bloodpressure', 'diabetic', 'children', 'smoker', 'region', 'claims']

# data = 'https://raw.githubusercontent.com/kenmanohar/stat6005/main/insurance_data.csv'
# df = pd.read_csv(data)

# # Drop rows with null values
# df.dropna(inplace=True)
# df.reset_index(drop=True, inplace=True)

# # Choose attributes
# #subset_columns = ['age', 'gender', 'bmi', 'bloodpressure', 'diabetic', 'children', 'smoker', 'region', 'claims']
# #subset_columns = ['age', 'gender', 'bmi', 'diabetic', 'children', 'smoker', 'region', 'claims']
# subset_columns = ['smoker', 'claims']

# df = df[subset_columns].copy()

# # Set label column
# label = 'claims'

# # Extract attributes
# attributes = df.columns.tolist()
# attributes.remove(label)

# Student Math dataset

data = 'https://raw.githubusercontent.com/kenmanohar/stat6005/main/student_math.csv'
df = pd.read_csv(data)

# Drop rows with null values
df.dropna(inplace=True)
df.reset_index(drop=True, inplace=True)

# subset_columns = ['G1', 'G2', 'failures', 'absences', 'age', 'G3']
subset_columns = ['G2', 'G3']
df = df[subset_columns].copy()

# Set target column
label = 'G3'

# Extract attributes
attributes = df.columns.tolist()
attributes.remove(label)

# Create train and test sets

train, test = train_test_split(df, train_size=0.8, random_state=0)
train.reset_index(inplace=True, drop=True)
test.reset_index(inplace=True, drop=True)

# a = subset_columns[0]
# kappa = 1

# train["r_avg_"+a] = df.apply(lambda x: calculate_averages(x, a, kappa), axis=1)
# quantative_averages = dict( zip(train[a], train["r_avg_"+a]) )

# test["r_avg_"+a] = test[a].map(quantative_averages)

# test["r_avg_"+a].isna().any()

# For a quantative attribute, the sample average is based on the values of the attribute

def calculate_averages(sample, a, kappa):

  global df, label

  distance = 0
  distance += (sample[a] - train[a]).abs()

  weights = 1 / (1 + distance)**kappa
  weighted_label = train[label] * weights

  sample_average = weighted_label.sum() / weights.sum()

  return sample_average

def impute_averages(row, a, kappa):
    if pd.isna(row["r_avg_"+a]):
        return calculate_averages(row, a, kappa)
    else:
        return row["r_avg_"+a]

# Find robust averages

def find_robust_averages(kappa):

  for a in attributes:

    # For a quantative attribute, the robust average is the category value
    if df[a].dtype == 'int64' or df[a].dtype == 'float64':
      #print(f"{a} is quantative")
      # # Version 1
      # train["r_avg_"+a] = train[a]
      # test["r_avg_"+a] = test[a]

      # # Version 2
      train["r_avg_"+a] = train.apply(lambda x: calculate_averages(x, a, kappa), axis=1)
      test["r_avg_"+a] = test.apply(lambda x: calculate_averages(x, a, kappa), axis=1)

      # Version 3
      # train["r_avg_"+a] = train.apply(lambda x: calculate_averages(x, a, kappa), axis=1)
      # quantative_averages = dict( zip(train[a], train["r_avg_"+a]) )
      # test["r_avg_"+a] = test[a].map(quantative_averages)
      # test["r_avg_"+a] = test.apply(lambda x: impute_averages(x, a, kappa), axis=1)

    # For a categorical attribute, the robust average is the mean in terms of the label
    else:
      #print(f"{a} is categorical")
      categorical_averages = train.groupby(a)[label].mean().to_dict()
      train["r_avg_"+a] = train[a].map(categorical_averages)
      test["r_avg_"+a] = test[a].map(categorical_averages)

def predict_label(test_sample, kappa):

  global df, train, attributes

  distance = 0

  for a in attributes:
    #distance += ( (test_sample["r_avg_"+a] - train["r_avg_"+a]) )**2

    # For a quantative attribute, the distance is the absolute difference
    if df[a].dtype == 'int64' or df[a].dtype == 'float64':
      distance += (test_sample["r_avg_"+a] - train["r_avg_"+a]).abs()

    # For a categorical attribute, the distance is the difference squared
    else:
      distance += ( (test_sample["r_avg_"+a] - train["r_avg_"+a]) )**2

  distance = distance**0.5
  weights = 1 / (1 + distance)**kappa
  weighted_label = train[label] * weights

  predicted_label = weighted_label.sum() / weights.sum()

  return predicted_label

def get_mse(kappa):

  global test

  find_robust_averages(kappa)

  test[f"predicted_{label}"] = test.apply(lambda x: predict_label(x, kappa), axis=1)

  mse = mean_squared_error(test[label], test[f"predicted_{label}"])

  return mse

def find_quadratic_coefficients(x_coor, y_coor):
  x, y  = np.array(x_coor), np.array(y_coor)

  # Define the coefficient matrix A and the dependent variable vector b
  A = np.array([[x[0]**2, x[0], 1], [x[1]**2, x[1], 1], [x[2]**2, x[2], 1]])
  b = np.array([y[0], y[1], y[2]])

  # Solve the system of equations using matrix algebra
  coeffs = np.linalg.solve(A, b)

  return coeffs[0], coeffs[1], coeffs[2]

# MSE when kappa = 0
mse0 = get_mse(0)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Grid search
# 
# start, stop = 0, 20
# num_steps  = 20
# step = (1/num_steps) * (stop - start)
# 
# results = pd.DataFrame(columns=("kappa", "mse"))
# 
# current_value = start
# while current_value <= stop:
#   results.loc[len(results.index)] = [current_value, get_mse(current_value) / mse0]
#   current_value += step
# 
# results

# Grid search results

index_mse_min = results["mse"].idxmin()
row_with_min_mse = results.iloc[index_mse_min]
row_with_min_mse

kappa_infinity = 15

train.columns[train.isna().any()].tolist()

test.columns[test.isna().any()].tolist()

# Generate 3 starting points

coor = pd.DataFrame(columns=("x", "y"))

coor.loc[len(coor.index)] = [0, 1]
coor.loc[len(coor.index)] = [kappa_infinity, get_mse(kappa_infinity) / mse0]
coor.loc[len(coor.index)] = [kappa_infinity/2, get_mse(kappa_infinity/2) / mse0]

coor.sort_values(by="x", ascending=True, inplace=True)
coor.reset_index(drop=True, inplace=True)

coor.drop(coor.index[-1])

coor

best_kappa, lowest_mse = coor.iloc[coor["y"].idxmin()]
best_kappa, lowest_mse

previous_x_min = best_kappa
epsilon = 1
threshold = 0.001

summary = pd.DataFrame(columns=("Step", "Kappas", "MSEs", "Shape", "Coefficient a", "Best Kappa", "Min Point"))

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Successive quadratic approximation
# # Stopping condition is minimal change in x_min i.e. minimum point on quadratic approximation curve
# 
# counter = 0
# while epsilon > threshold:
#   summary.loc[len(summary.index)] = [counter, [coor["x"].round(4)], [coor["y"].round(4)], "", [], (round(best_kappa,4), round(lowest_mse,4)), []]
# 
#   print(f"Step {counter}")
#   print(coor)
#   print()
# 
#   x1, y1 = coor.iloc[0]
#   x2, y2 = coor.iloc[1]
#   x3, y3 = coor.iloc[2]
# 
#   # Determine quadratic coefficients
#   a, b, c = find_quadratic_coefficients([x1, x2, x3], [y1, y2, y3])
#   summary.loc[counter, "Coefficient a"] = a
# 
#   # Find minimum point on quadratic curve
#   x_min = (-1*b) / (2*a)
#   y_min = get_mse(x_min) / mse0
#   summary.loc[counter, "Min Point"] = x_min
# 
#   # Update stopping condition
#   epsilon = abs(previous_x_min - x_min)
#   previous_x_min = x_min
# 
#   # If convex, minimum point lies within this convex region
#   # Find coordinate with highest y value and replace with minimum point
#   if a > 0:
#     summary.loc[counter, "Shape"] = "convex"
#     coor.loc[len(coor.index)] = [x_min, y_min] # Add minimum point to coor
#     coor.sort_values(by="y", ascending=True, inplace=True) # Sort in ascending order by y value
#     coor.reset_index(drop=True, inplace=True) # Reset index
#     coor.drop(coor.index[-1], inplace=True) # Drop last row
#     coor.sort_values(by="x", ascending=True, inplace=True) # Sort in ascending order by x value
#     coor.reset_index(drop=True, inplace=True) # Reset index
# 
#   # If concave, minimum point lies to the left of the point with the lowest y value
#   # Reset from the lowest x value
#   else:
#     summary.loc[counter, "Shape"] = "concave"
#     temp = coor.at[0, "x"]
#     coor.drop(coor.index[-2:], inplace=True) # Drop last 2 rows and keep the point with the leftmost x value
#     coor.loc[len(coor.index)] = [0, 1] # Add point at x = 0
#     coor.loc[len(coor.index)] = [temp/2, get_mse(temp/2) / mse0] # Add midway point
#     coor.sort_values(by="x", ascending=True, inplace=True) # Sort in ascending order by x value
#     coor.reset_index(drop=True, inplace=True) # Reset index
# 
#   # Update best kappa and step counter
#   best_kappa, lowest_mse = coor.iloc[coor["y"].idxmin()]
#   counter += 1

summary

#print(f"Attribute(s): {attributes}")

# if get_mse(0.5) >= 1: print(f"{attributes} is not useful")
# else: print(f"{attributes} is useful")

if best_kappa < 0.5: print(f"{attributes} is not useful\n")
else: print(f"{attributes} is useful\n")

print(f"Grid Search Kappa: {row_with_min_mse[0]} with MSE {row_with_min_mse[1]}\n")

print(f"Optimal Kappa: {best_kappa} with MSE {lowest_mse}\n")

if row_with_min_mse[1] < lowest_mse: print("Grid Search Kappa has lower MSE")
if row_with_min_mse[1] > lowest_mse: print("Optimal Kappa has lower MSE")
if row_with_min_mse[1] == lowest_mse: print("Grid Search Kappa and Optimial Kappa have same MSE")

"""<!--  -->

<!--  -->
"""